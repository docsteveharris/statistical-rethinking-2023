{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770d07f64dbd473f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T14:50:59.979996Z",
     "start_time": "2024-01-14T14:50:59.973769Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d941b230baefd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "via Ben Lambert\n",
    "> the first and most important choice in Bayesian (or Frequentist) analysis is which probability model to use to describe a given process\n",
    "\n",
    "This statement is helpful because it makes clear the centrality of the choice of probability model. With coin tossing, we can justify this from the physical parallels. With biologically, political, or social models then we try to choose the model that imposes as few assumptions as possible. This is the concept of maximum entropy, and justifies why the binomial distribution is reasonable for binary choices, and the normal distribution is reasonable for processes that sum. But note the normal distribution carries risk because the tails are too narrow. In other words, it assumes the additive process is a complete description whereas in reality there are other unknown inputs that mean thicker tails are a better choice.\n",
    "\n",
    "Working forwards in a generative manner, the parameters are fixed and the resultant data behaves as a valid probability distribution. But working backwards and fixing the data, then the resultant parameter distribution is not a valid probability distribution. When employing the function in this direction, we call it a *likelihood function* to make this distinction clear. Discrete likelihood distributions do not sum to `1`, and continuous likelihood distributions do not integrate to `1` but both are driven by the same *probability model*, and when run in reverse (as a generative model) they do (sum/integrate to `1`).\n",
    "\n",
    "The implication is that the analyst must consider the mechanism underlying the data generating process.\n",
    "\n",
    "And whilst we typically write \n",
    "\n",
    "$$\n",
    "p(\\theta|data) = \\frac{p(data|\\theta) \\times p(\\theta)}{p(data)}\n",
    "$$\n",
    "\n",
    "we should write ...\n",
    "\n",
    "$$\n",
    "p(\\theta|data,model) = \\frac{p(data|\\theta) \\times p(\\theta) \\times p(model)}{p(data,model)}\n",
    "$$\n",
    "\n",
    "but whilst we drop the $p(model)$ term when writing, we should always remember that our conclusions depend on the probability model chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b910561d8cdc6f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The equivalence relation\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta|data) = p(data|\\theta)\n",
    "$$\n",
    "\n",
    "> We call the above the *equivalence relation* since a likelihood of $\\theta$ for a particular data sample is equivalent to the probability of that data sample for that value of $\\theta$.\n",
    "\n",
    "Let's make that concrete by considering a simple coin flip, where the coin is biased, and the probabilty of obtaining heads on any throw is one of 6 discrete values $\\theta \\in \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$. We now flip the coin twice. This will produce the following table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ebcd0c6356fb3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T14:51:03.140637Z",
     "start_time": "2024-01-14T14:51:03.122726Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th><th>column_2</th></tr><tr><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.64</td><td>0.32</td><td>0.04</td></tr><tr><td>0.36</td><td>0.48</td><td>0.16</td></tr><tr><td>0.16</td><td>0.48</td><td>0.36</td></tr><tr><td>0.04</td><td>0.32</td><td>0.64</td></tr><tr><td>0.0</td><td>0.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 3)\n",
       "┌──────────┬──────────┬──────────┐\n",
       "│ column_0 ┆ column_1 ┆ column_2 │\n",
       "│ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════╪══════════╡\n",
       "│ 1.0      ┆ 0.0      ┆ 0.0      │\n",
       "│ 0.64     ┆ 0.32     ┆ 0.04     │\n",
       "│ 0.36     ┆ 0.48     ┆ 0.16     │\n",
       "│ 0.16     ┆ 0.48     ┆ 0.36     │\n",
       "│ 0.04     ┆ 0.32     ┆ 0.64     │\n",
       "│ 0.0      ┆ 0.0      ┆ 1.0      │\n",
       "└──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write steps and flips with + 1 as a reminder that python is 0-indexed\n",
    "steps = 5 + 1\n",
    "flips = 2 \n",
    "thetas = np.linspace(0.0, 1.0, num=steps)\n",
    "\n",
    "probs = []\n",
    "for theta in thetas:\n",
    "    for heads in range(flips + 1):\n",
    "        tails = flips - heads\n",
    "        p = math.comb(flips, heads) * (theta ** heads) * (1.0 - theta) ** tails\n",
    "        p = np.round(p, 4) # b/c of floating point rounding errors\n",
    "        probs.append(p)\n",
    "probs = np.array(probs).reshape(steps, flips + 1)\n",
    "probs = pl.DataFrame(probs)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b61d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>0 heads</th><th>1 heads</th><th>2 heads</th><th>thetas</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.64</td><td>0.32</td><td>0.04</td><td>0.2</td></tr><tr><td>0.36</td><td>0.48</td><td>0.16</td><td>0.4</td></tr><tr><td>0.16</td><td>0.48</td><td>0.36</td><td>0.6</td></tr><tr><td>0.04</td><td>0.32</td><td>0.64</td><td>0.8</td></tr><tr><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 4)\n",
       "┌─────────┬─────────┬─────────┬────────┐\n",
       "│ 0 heads ┆ 1 heads ┆ 2 heads ┆ thetas │\n",
       "│ ---     ┆ ---     ┆ ---     ┆ ---    │\n",
       "│ f64     ┆ f64     ┆ f64     ┆ f64    │\n",
       "╞═════════╪═════════╪═════════╪════════╡\n",
       "│ 1.0     ┆ 0.0     ┆ 0.0     ┆ 0.0    │\n",
       "│ 0.64    ┆ 0.32    ┆ 0.04    ┆ 0.2    │\n",
       "│ 0.36    ┆ 0.48    ┆ 0.16    ┆ 0.4    │\n",
       "│ 0.16    ┆ 0.48    ┆ 0.36    ┆ 0.6    │\n",
       "│ 0.04    ┆ 0.32    ┆ 0.64    ┆ 0.8    │\n",
       "│ 0.0     ┆ 0.0     ┆ 1.0     ┆ 1.0    │\n",
       "└─────────┴─────────┴─────────┴────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_names = [\"0 heads\", \"1 heads\", \"2 heads\"]\n",
    "probs = probs.select([pl.col(old).alias(new) for old, new in zip(probs.columns, new_names)])\n",
    "probs.with_columns(pl.Series(name=\"thetas\", values=thetas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c0ac4ddfc4923",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that each row is a valid probability distribution that sums to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b13896516697507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T14:52:39.754406Z",
     "start_time": "2024-01-14T14:52:39.749413Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sum</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>1.0</td></tr><tr><td>1.0</td></tr><tr><td>1.0</td></tr><tr><td>1.0</td></tr><tr><td>1.0</td></tr><tr><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6,)\n",
       "Series: 'sum' [f64]\n",
       "[\n",
       "\t1.0\n",
       "\t1.0\n",
       "\t1.0\n",
       "\t1.0\n",
       "\t1.0\n",
       "\t1.0\n",
       "]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.select(pl.all()).sum_horizontal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7237cfc85fc6d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But each column is a likelihood function not a probability distribution because we vary the parameter $\\theta$, and the sum of the likelihood functions is not equal to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72e98a2a45e2a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T14:55:38.942152Z",
     "start_time": "2024-01-14T14:55:38.932833Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>0 heads</th><th>1 heads</th><th>2 heads</th></tr><tr><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2.2</td><td>1.6</td><td>2.2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "┌─────────┬─────────┬─────────┐\n",
       "│ 0 heads ┆ 1 heads ┆ 2 heads │\n",
       "│ ---     ┆ ---     ┆ ---     │\n",
       "│ f64     ┆ f64     ┆ f64     │\n",
       "╞═════════╪═════════╪═════════╡\n",
       "│ 2.2     ┆ 1.6     ┆ 2.2     │\n",
       "└─────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sum expression is applied by column by default (hence why we used the sum_horizontal function) in the example above\n",
    "probs.select(pl.all()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedaf5d1070e359",
   "metadata": {
    "collapsed": false
   },
   "source": [
    ":::{.callout-important}\n",
    "The take home message is that $p(\\theta|data)$ is not a valid probability distribution.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a92cefd51a8c1dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:35:19.617039Z",
     "start_time": "2024-01-11T22:35:19.614417Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_likelihood_binom(n, k, probs):\n",
    "    \"\"\"\n",
    "    Returns standardised likelihoods\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        number of trials (assumed constant)\n",
    "    k : list(int)\n",
    "        list of number of successes in each trial\n",
    "    probs: np.ndarray\n",
    "        The range of probabilities being evaluated\n",
    "    \"\"\"\n",
    "    likelihoods = []\n",
    "    for prob in probs:\n",
    "        likelihood = np.prod(binom.pmf(k, n, prob))\n",
    "        likelihoods.append(likelihood)\n",
    "    res = pl.Series(likelihoods)\n",
    "    return res / res.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:35:57.127332Z",
     "start_time": "2024-01-11T22:35:57.123841Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (11,)\n",
      "Series: '' [f64]\n",
      "[\n",
      "\t0.0\n",
      "\t3.5405e-42\n",
      "\t3.8709e-25\n",
      "\t9.8502e-16\n",
      "\t1.4153e-9\n",
      "\t0.000024\n",
      "\t0.015649\n",
      "\t0.515843\n",
      "\t0.467961\n",
      "\t0.000523\n",
      "\t0.0\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n, k = 10, [6,7,8,9,6,7,8,9] # trials, successes\n",
    "probs = np.linspace(0, 1, 10+1)\n",
    "\n",
    "res = calculate_likelihood_binom(n, k, probs)\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9ced36c5b871ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:28:28.836151Z",
     "start_time": "2024-01-11T22:28:28.833381Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3ac679b6a9fea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Miele 9917710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f7ff7bfc2e39a5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:41:31.769030Z",
     "start_time": "2024-01-11T22:41:31.765506Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_likelihood_norm(obs, mean_grid, std_grid):\n",
    "    \"\"\"\n",
    "    Perform a grid search across different possible standard deviations and mean values\n",
    "    At each point calculate the probability density for the observed data by multiplying together the probabilities for each data point\n",
    "    \"\"\"\n",
    "    likelihoods = []\n",
    "    for std in std_grid:\n",
    "        for mean in mean_grid:\n",
    "            likelihood = np.prod(norm.pdf(obs, mean, std))\n",
    "            likelihoods.append(likelihood)\n",
    "    res = pl.Series(likelihoods)\n",
    "    return res / res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9972e1c0b632880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:10:17.711075Z",
     "start_time": "2024-01-11T23:10:17.705749Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimate the normal distribution for height\n",
    "obs = [183, 165, 182, 140]\n",
    "mean_grid = np.linspace(100, 200, 10+1)\n",
    "std_grid = np.linspace(10,50, 5)\n",
    "\n",
    "res = calculate_likelihood_norm(obs, mean_grid, std_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2a299751352e7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:03:53.118270Z",
     "start_time": "2024-01-11T23:03:53.114357Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "       [20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.],\n",
       "       [30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30.],\n",
       "       [40., 40., 40., 40., 40., 40., 40., 40., 40., 40., 40.],\n",
       "       [50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, s = np.meshgrid(mean_grid, std_grid)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7661092cdf5922f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T22:47:33.083475Z",
     "start_time": "2024-01-11T22:47:33.080628Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00017226, 0.01295176])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf([183, 165], 150, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d00ee27c9f5c5e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:15:59.046924Z",
     "start_time": "2024-01-11T23:15:59.042329Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = np.array([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bfa4bcd2e4be63ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T23:16:23.502717Z",
     "start_time": "2024-01-11T23:16:23.500087Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8318ac2b5ebb3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
